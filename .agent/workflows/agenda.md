---
description: Next set of agendas for completing the Erasus framework based on the specification gap analysis
---

# Erasus Framework â€” Next Agenda (Gap Analysis vs. Specification)

**Date:** 2026-02-14
**Source:** `erasus_complete_comprehensive_specification.txt`

---

## âœ… What's DONE (Phase 1 Complete)

### Core Infrastructure
- `core/` â€” `base_unlearner.py`, `base_selector.py`, `base_strategy.py`, `base_metric.py`, `config.py`, `registry.py`, `exceptions.py`, `types.py` â€” **All implemented**

### Models (15+ Architectures target â†’ 8 implemented)
- `models/vlm/clip.py` âœ… | `llava.py` âœ… | `blip.py` âœ…
- `models/llm/llama.py` âœ… | `mistral.py` âœ… | `gpt.py` âœ… | `bert.py` âœ…
- `models/diffusion/stable_diffusion.py` âœ…
- `models/audio/whisper.py` âœ…
- `models/video/videomae.py` âœ…

### Selectors (25+ target â†’ 15+ implemented)
- `gradient_based/` â€” `influence.py` âœ… | `tracin.py` âœ… | `gradient_norm.py` âœ… | `grad_match.py` âœ… | `el2n.py` âœ… | `representer.py` âœ…
- `geometry_based/` â€” `kcenter.py` âœ… | `herding.py` âœ… | `craig.py` âœ… | `glister.py` âœ… | `submodular.py` âœ… | `kmeans_coreset.py` âœ…
- `learning_based/` â€” `forgetting_events.py` âœ… | `data_shapley.py` âœ… | `valuation_network.py` âœ…
- `ensemble/voting.py` âœ… | `auto_selector.py` âœ… | `random_selector.py` âœ… | `full_selector.py` âœ…

### Strategies (20+ target â†’ 17+ implemented)
- `gradient_methods/` â€” `gradient_ascent.py` âœ… | `scrub.py` âœ… | `modality_decoupling.py` âœ… | `fisher_forgetting.py` âœ… | `negative_gradient.py` âœ…
- `parameter_methods/` â€” `lora_unlearning.py` âœ… | `sparse_aware.py` âœ… | `mask_based.py` âœ… | `neuron_pruning.py` âœ…
- `data_methods/` â€” `amnesiac.py` âœ… | `sisa.py` âœ… | `certified_removal.py` âœ…
- `llm_specific/` â€” `ssd.py` âœ… | `token_masking.py` âœ… | `embedding_alignment.py` âœ… | `causal_tracing.py` âœ…
- `diffusion_specific/` â€” `concept_erasure.py` âœ… | `noise_injection.py` âœ… | `unet_surgery.py` âœ…
- `vlm_specific/` â€” `contrastive_unlearning.py` âœ…

### Losses
- `retain_anchor.py` âœ… | `contrastive.py` âœ… | `kl_divergence.py` âœ… | `mmd.py` âœ… | `custom_losses.py` âœ…

### Metrics
- `accuracy.py` âœ… | `membership_inference.py` âœ… | `perplexity.py` âœ… | `retrieval.py` âœ… | `fid.py` âœ…

### Privacy
- `accountant.py` âœ… | `dp_mechanisms.py` âœ… | `certificates.py` âœ… | `influence_bounds.py` âœ…

### Utils / Data / Config
- `checkpointing.py` âœ… | `logging.py` âœ… | `seed.py` âœ…
- `loaders.py` âœ… | `datasets.py` âœ… | `multimodal.py` âœ… | `splits.py` âœ… | `transforms.py` âœ…

---

## ğŸ”´ GAPS â€” What's MISSING vs. Specification

### PRIORITY 1 â€” High-Level Unlearner API (Critical for usability)
The spec (Section 9, 10) shows a high-level `ErasusUnlearner.fit()` API. Currently:
- `erasus/unlearners/erasus_unlearner.py` exists (2.4KB) â€” **needs audit**
- **MISSING**: `vlm_unlearner.py`, `llm_unlearner.py`, `diffusion_unlearner.py`, `audio_unlearner.py`, `video_unlearner.py`, `multimodal_unlearner.py`
- These are the user-facing orchestration classes that tie selector â†’ strategy â†’ metric together

### PRIORITY 2 â€” Visualization Module (Spec Section: visualization/)
Currently only 3 files exist:
- `loss_curves.py` âœ… | `feature_plots.py` âœ… | `mia_plots.py` âœ…
- **MISSING**: `embeddings.py`, `surfaces.py`, `gradients.py`, `reports.py`, `interactive.py`
- These should provide t-SNE/PCA embedding plots, loss landscape surfaces, gradient flow visualization, HTML report generation, and interactive dashboards

### PRIORITY 3 â€” Metrics Module Restructuring (Spec Section 6)
The specification defines a much richer metrics hierarchy:
- **MISSING** `metrics/metric_suite.py` â€” Unified metric runner
- **MISSING** `metrics/forgetting/` directory:
  - `mia.py` â€” Full blown MIA with ROC curves (current `membership_inference.py` is flat, not in subfolder)
  - `mia_variants.py` â€” LiRA and other advanced attacks
  - `confidence.py` â€” Confidence-based forgetting measures
  - `feature_distance.py` â€” Embedding distance metrics
- **MISSING** `metrics/utility/` directory (currently flat)
- **MISSING** `metrics/efficiency/`:
  - `time_complexity.py` â€” Wall-clock and FLOPs tracking
  - `memory_usage.py` â€” Peak memory, GPU utilization
- **MISSING** `metrics/privacy/`:
  - `differential_privacy.py` â€” DP-specific evaluation metrics

### PRIORITY 4 â€” Benchmark Datasets (Spec Section 5)
- **MISSING** `data/datasets/` directory entirely:
  - `coco.py` â€” COCO Captions dataset wrapper
  - `conceptual_captions.py` â€” CC3M/CC12M wrapper
  - `tofu.py` â€” TOFU benchmark (critical for LLM eval)
  - `wmdp.py` â€” WMDP benchmark
  - `i2p.py` â€” Inappropriate Image Prompts for diffusion
- **MISSING** `data/synthetic/backdoor_generator.py`
- **MISSING** `data/preprocessing.py`, `data/partitioning.py`, `data/samplers.py`

### PRIORITY 5 â€” CLI Module (Spec Section: cli/)
- `cli/main.py` exists (1.5KB) â€” **needs audit**
- **MISSING**: `cli/unlearn.py`, `cli/evaluate.py`
- These enable `erasus unlearn --config config.yaml` and `erasus evaluate` commands

### PRIORITY 6 â€” VLM-Specific Strategy Gap
- `vlm_specific/cross_modal_decoupling.py` is only 214 bytes â€” **likely a stub/alias**

### PRIORITY 7 â€” Certification Module (Spec Section 7.2)
The specification defines `certification/` as separate from `privacy/`:
- **MISSING**: `certification/` directory:
  - `certified_removal.py`
  - `verification.py`
- Note: the privacy folder has `certificates.py` and `influence_bounds.py` but these may not cover formal verification

### PRIORITY 8 â€” Experiments Module
- **MISSING** `experiments/` directory:
  - `experiment_tracker.py` â€” W&B / MLflow integration for tracking runs

### PRIORITY 9 â€” Project Infrastructure
- **MISSING** `configs/` directory with YAML presets:
  - `models/clip.yaml`, `llama.yaml`, `stable_diffusion.yaml`
  - `selectors/influence.yaml`, `craig.yaml`, `auto.yaml`
  - `strategies/gradient_ascent.yaml`, `modality_decoupling.yaml`, `scrub.yaml`
  - `default.yaml`
- **MISSING** `scripts/` â€” `setup_env.sh`, `download_datasets.py`, `run_benchmarks.sh`
- **MISSING** `benchmarks/` â€” `tofu/run.py`, `muse/run.py`, `wmdp/run.py`
- **MISSING** `examples/` â€” Only `clip_basic.py` exists. Need:
  - `vision_language/clip_coreset_comparison.py`, `llava_unlearning.py`, `blip_unlearning.py`
  - `language_models/llama_concept_removal.py`, `gpt2_unlearning.py`, `lora_efficient_unlearning.py`
  - `diffusion_models/stable_diffusion_artist.py`, `stable_diffusion_nsfw.py`
  - `benchmarks/run_tofu_benchmark.py`
- **MISSING** `docs/` â€” Documentation with Sphinx/RST
- **MISSING** `.github/workflows/` â€” CI/CD
- **MISSING** `docker/` â€” Dockerfiles
- **MISSING** `papers/reproductions/` â€” Paper reproduction scripts
- **MISSING** `utils/distributed.py`, `utils/helpers.py`

### PRIORITY 10 â€” Testing
Current tests are minimal (5 files, ~12KB). Spec requires:
- **MISSING** `tests/conftest.py` â€” Shared fixtures
- **MISSING** `tests/unit/test_selectors.py`, `test_strategies.py`, `test_metrics.py`
- **MISSING** `tests/integration/` â€” `test_clip_pipeline.py`, `test_llm_pipeline.py`, `test_diffusion_pipeline.py`
- **MISSING** `tests/benchmarks/test_performance.py`

---

## ğŸ“‹ RECOMMENDED SPRINT PLAN

### Sprint 1 â€” Unlearners + CLI (User-Facing API) âš¡
**Goal:** Make the framework usable end-to-end from a single entry point
1. Audit and complete `erasus_unlearner.py`
2. Create `vlm_unlearner.py`, `llm_unlearner.py`, `diffusion_unlearner.py`
3. Complete `cli/unlearn.py` and `cli/evaluate.py`
4. Create `configs/default.yaml` and model-specific YAML configs
5. Write an end-to-end integration test

### Sprint 2 â€” Visualization Module ğŸ“Š
**Goal:** Complete all visualization capabilities
1. Implement `embeddings.py` â€” t-SNE/PCA plots of forget/retain embeddings
2. Implement `surfaces.py` â€” Loss landscape visualization
3. Implement `gradients.py` â€” Gradient flow and magnitude plots
4. Implement `reports.py` â€” HTML report generator with all metrics/plots
5. Implement `interactive.py` â€” Plotly/Dash interactive dashboard

### Sprint 3 â€” Metrics Restructuring + New Metrics ğŸ“
**Goal:** Match the 50+ metrics target from the spec
1. Create `metrics/metric_suite.py` â€” Unified runner
2. Create `metrics/forgetting/` â€” `mia.py`, `mia_variants.py`, `confidence.py`, `feature_distance.py`
3. Create `metrics/efficiency/` â€” `time_complexity.py`, `memory_usage.py`
4. Create `metrics/privacy/differential_privacy.py`
5. Refactor existing flat metrics into the hierarchy

### Sprint 4 â€” Benchmark Datasets ğŸ“¦
**Goal:** Make TOFU, WMDP, I2P, COCO usable out of the box
1. Create `data/datasets/tofu.py` â€” TOFU benchmark loader
2. Create `data/datasets/wmdp.py` â€” WMDP benchmark loader
3. Create `data/datasets/coco.py` â€” COCO Captions loader
4. Create `data/datasets/i2p.py` â€” I2P prompts loader
5. Create `data/preprocessing.py`, `data/samplers.py`

### Sprint 5 â€” Examples + Benchmarks + Documentation ğŸ“š
**Goal:** Make the framework approachable
1. Write 8+ example scripts (CLIP, LLaVA, LLaMA, GPT-2, Stable Diffusion)
2. Create benchmark runners (`benchmarks/tofu/`, `benchmarks/wmdp/`)
3. Create `papers/reproductions/` scripts
4. Add comprehensive docstrings across all modules
5. Set up Sphinx documentation skeleton

### Sprint 6 â€” Testing + CI/CD + Docker ğŸ§ª
**Goal:** Production-readiness
1. Create `tests/conftest.py` with shared fixtures
2. Write unit tests for all selectors, strategies, metrics
3. Write integration tests for CLIP, LLM, Diffusion pipelines
4. Set up `.github/workflows/ci.yml`
5. Create `docker/Dockerfile` and `docker-compose.yml`

---

## ğŸ¯ IMMEDIATE NEXT SESSION AGENDA

For the next coding session, focus on **Sprint 1** (most impactful):

1. **Audit `erasus_unlearner.py`** â€” Verify the high-level `.fit()` API works
2. **Create `vlm_unlearner.py`** â€” CLIP/LLaVA/BLIP orchestration
3. **Create `llm_unlearner.py`** â€” LLaMA/GPT/Mistral orchestration
4. **Create `diffusion_unlearner.py`** â€” Stable Diffusion concept erasure orchestration
5. **Complete `cli/unlearn.py`** â€” CLI command to run unlearning from terminal
6. **Complete `cli/evaluate.py`** â€” CLI command to evaluate results
7. **Create YAML configs** â€” `configs/default.yaml`, `configs/models/*.yaml`
8. **Write integration test** â€” End-to-end test with a tiny model
