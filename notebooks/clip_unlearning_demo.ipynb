{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \ud83d\uddbc\ufe0f CLIP Unlearning: Removing Concepts from Vision-Language Models\n",
                "\n",
                "**Erasus Framework \u2014 VLM Unlearning**\n",
                "\n",
                "This notebook demonstrates how to remove specific visual concepts (e.g., \"dog\") from a CLIP model using the **Modality Decoupling** strategy. This technique decorrelates the image and text embeddings for the target concept while preserving other capabilities.\n",
                "\n",
                "## What You\u2019ll Learn\n",
                "\n",
                "1. **Setup**: Load CLIP (or a lightweight demo version)\n",
                "2. **Define Concepts**: Identify concepts to forget (e.g., \"a photo of a dog\") and retain (e.g., \"a photo of a cat\")\n",
                "3. **Unlearn**: Apply gradient ascent on the cosine similarity between image and text embeddings\n",
                "4. **Verify**: Measure the drop in zero-shot classification accuracy for the forgotten concept\n",
                "\n",
                "---\n",
                "\n",
                "### Modes\n",
                "\n",
                "- **Demo Mode** (Default): Uses `MiniCLIP` shim. Runs in seconds on CPU. Good for verifying the API.\n",
                "- **Real Mode**: Uses `openai/clip-vit-base-patch32`. Requires internet to download model (~600MB). Set `USE_REAL_MODEL = True`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Install dependencies\n",
                "# !pip install -q erasus transformers torch matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Configuration\n",
                "USE_REAL_MODEL = False\n",
                "REAL_MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
                "LEARNING_RATE = 1e-4\n",
                "EPOCHS = 5"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Imports\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "\n",
                "from erasus.unlearners import VLMUnlearner\n",
                "import erasus.strategies\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Model (Real or Mini)\n",
                "\n",
                "We use a `MiniCLIP` for instant execution, or load real CLIP from HuggingFace."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: MiniCLIP shim\n",
                "class MiniCLIP(nn.Module):\n",
                "    \"\"\"Minimal CLIP implementation for demo purposes.\"\"\"\n",
                "    def __init__(self, embed_dim=32, vocab_size=100):\n",
                "        super().__init__()\n",
                "        self.visual = nn.Sequential(\n",
                "            nn.Linear(3*224*224, 64),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(64, embed_dim)\n",
                "        )\n",
                "        self.text_model = nn.Sequential(\n",
                "            nn.Embedding(vocab_size, 64),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(64, embed_dim)\n",
                "        )\n",
                "        self.logit_scale = nn.Parameter(torch.ones([]) * 2.6592)\n",
                "        self.dummy_vocab = {f\"word_{i}\": i for i in range(vocab_size)}\n",
                "\n",
                "    def get_image_features(self, pixel_values):\n",
                "        B = pixel_values.shape[0]\n",
                "        x = pixel_values.view(B, -1)\n",
                "        return F.normalize(self.visual(x), dim=-1)\n",
                "\n",
                "    def get_text_features(self, input_ids):\n",
                "        x = input_ids.mean(dim=1).long()\n",
                "        # Use a simpler approach for dummy embedding: mean of embeddings\n",
                "        x = self.text_model[0](input_ids).mean(dim=1)\n",
                "        x = self.text_model[2](F.relu(x))\n",
                "        return F.normalize(x, dim=-1)\n",
                "\n",
                "    def forward(self, pixel_values, input_ids, **kwargs):\n",
                "        img_emb = self.get_image_features(pixel_values)\n",
                "        txt_emb = self.get_text_features(input_ids)\n",
                "        scale = self.logit_scale.exp()\n",
                "        logits_per_image = scale * img_emb @ txt_emb.t()\n",
                "        logits_per_text = logits_per_image.t()\n",
                "        return type(\"Out\", (), {\"logits_per_image\": logits_per_image, \"logits_per_text\": logits_per_text})()\n",
                "\n",
                "def load_model():\n",
                "    if USE_REAL_MODEL:\n",
                "        from transformers import CLIPModel, CLIPProcessor\n",
                "        model = CLIPModel.from_pretrained(REAL_MODEL_ID).to(device)\n",
                "        processor = CLIPProcessor.from_pretrained(REAL_MODEL_ID)\n",
                "        return model, processor\n",
                "    else:\n",
                "        model = MiniCLIP()\n",
                "        # Dummy processor\n",
                "        class MiniProcessor:\n",
                "            def __call__(self, text=None, images=None, return_tensors=\"pt\", padding=True, truncation=True):\n",
                "                res = {}\n",
                "                if text:\n",
                "                    # Dummy tokenization\n",
                "                    ids = torch.randint(0, 100, (len(text), 5))\n",
                "                    res[\"input_ids\"] = ids\n",
                "                if images:\n",
                "                    # Dummy pixel values if images provided (lists of PIL or arrays)\n",
                "                    # Expecting mock tensors for MiniCLIP though\n",
                "                    res[\"pixel_values\"] = torch.randn(len(images), 3, 224, 224)\n",
                "                return res\n",
                "        return model.to(device), MiniProcessor()\n",
                "\n",
                "model, processor = load_model()\n",
                "print(\"Model loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prepare Data\n",
                "\n",
                "We create a Forget Set (images of dogs + text \"a photo of a dog\") and a Retain Set (images of cats + text \"a photo of a cat\").\n",
                "\n",
                "For the demo, we generate synthetic data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Create synthetic data\n",
                "def make_dummy_data(n_samples=10, concept=\"dog\"):\n",
                "    # Random noise for images\n",
                "    images = torch.randn(n_samples, 3, 224, 224)\n",
                "    \n",
                "    # Tokenize text\n",
                "    text = [f\"a photo of a {concept}\"] * n_samples\n",
                "    inputs = processor(text=text, return_tensors=\"pt\", padding=True)\n",
                "    input_ids = inputs[\"input_ids\"]\n",
                "    \n",
                "    # Create dataset: (image, input_ids)\n",
                "    # Erasus expects dictionary or tuple. Let's use tuple for simplicity if supported,\n",
                "    # or custom collate. VLMUnlearner handles dicts best.\n",
                "    return images, input_ids\n",
                "\n",
                "forget_imgs, forget_txt = make_dummy_data(50, \"dog\")\n",
                "retain_imgs, retain_txt = make_dummy_data(50, \"cat\")\n",
                "\n",
                "# Custom dataset that returns dicts\n",
                "class VLMDataset(torch.utils.data.Dataset):\n",
                "    def __init__(self, images, input_ids):\n",
                "        self.images = images\n",
                "        self.input_ids = input_ids\n",
                "    def __len__(self): return len(self.images)\n",
                "    def __getitem__(self, idx):\n",
                "        return {\"pixel_values\": self.images[idx], \"input_ids\": self.input_ids[idx]}\n",
                "\n",
                "forget_loader = DataLoader(VLMDataset(forget_imgs, forget_txt), batch_size=8)\n",
                "retain_loader = DataLoader(VLMDataset(retain_imgs, retain_txt), batch_size=8)\n",
                "\n",
                "print(f\"Forget set: {len(forget_loader.dataset)} items\")\n",
                "print(f\"Retain set: {len(retain_loader.dataset)} items\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Unlearn\n",
                "\n",
                "We use `VLMUnlearner` with the `gradient_ascent` strategy (or `modality_decoupling` if available)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: Run unlearning\n",
                "print(\"Starting unlearning...\")\n",
                "unlearner = VLMUnlearner(\n",
                "    model=model,\n",
                "    strategy=\"gradient_ascent\",\n",
                "    device=device,\n",
                "    strategy_kwargs={\"lr\": LEARNING_RATE}\n",
                ")\n",
                "\n",
                "result = unlearner.fit(\n",
                "    forget_data=forget_loader,\n",
                "    retain_data=retain_loader,\n",
                "    epochs=EPOCHS\n",
                ")\n",
                "\n",
                "print(f\"Unlearning complete in {result.elapsed_time:.2f}s\")\n",
                "if result.forget_loss_history:\n",
                "    plt.plot(result.forget_loss_history, label=\"Forget Loss\")\n",
                "    plt.title(\"Unlearning Curve\")\n",
                "    plt.xlabel(\"Steps\")\n",
                "    plt.ylabel(\"Loss (Higher = More Forgotten)\")\n",
                "    plt.legend()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluation\n",
                "\n",
                "We check if the similarity between \"dog\" images and text has decreased."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Evaluate similarity\n",
                "def get_similarity(model, loader):\n",
                "    sims = []\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        for batch in loader:\n",
                "            pix = batch[\"pixel_values\"].to(device)\n",
                "            ids = batch[\"input_ids\"].to(device)\n",
                "            out = model(pix, ids)\n",
                "            # Diagonal contains image-text pairs\n",
                "            logits = out.logits_per_image.diag()\n",
                "            sims.extend(logits.cpu().numpy())\n",
                "    return np.mean(sims)\n",
                "\n",
                "dog_score = get_similarity(model, forget_loader)\n",
                "cat_score = get_similarity(model, retain_loader)\n",
                "\n",
                "print(f\"Dog Similarity (should be low): {dog_score:.4f}\")\n",
                "print(f\"Cat Similarity (should be high): {cat_score:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.13.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}