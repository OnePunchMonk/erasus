{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcbb Copyright Removal: Erasing Proprietary Code from LLMs\n",
    "\n",
    "**Erasus Framework \u2014 Code Unlearning**\n",
    "\n",
    "This notebook demonstrates how to remove specific copyrighted code snippets or proprietary algorithms from a Code LLM (like StarCoder, CodeGen, or CodeLlama) while preserving its ability to write general code.\n",
    "\n",
    "## Scenario\n",
    "\n",
    "Imagine your model was accidentally trained on a proprietary algorithm: `fast_inverse_square_root_legacy`. You want the model to **forget** this specific implementation but **retain** knowledge of general Python programming.\n",
    "\n",
    "## Strategy\n",
    "\n",
    "We use **Gradient Ascent** on the specific code tokens, effectively maximizing the loss for reproducing that exact sequence."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from erasus.unlearners import LLMUnlearner\n",
    "import erasus.strategies  # register strategies\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mock Code Model\n",
    "\n",
    "We use a `MiniCodeGPT` for demonstration. Set `USE_REAL_MODEL = True` to load `Salesforce/codegen-350M-mono`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Model Definition\n",
    "USE_REAL_MODEL = False\n",
    "\n",
    "class MiniCodeGPT(nn.Module):\n",
    "    \"\"\"Tiny GPT-like model for code.\"\"\"\n",
    "    def __init__(self, vocab_size=1000, n_embd=64):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = nn.Embedding(512, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd), nn.ReLU(),\n",
    "            nn.Linear(n_embd, n_embd)\n",
    "        )\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, input_ids, labels=None, **kwargs):\n",
    "        B, T = input_ids.shape\n",
    "        tok = self.token_emb(input_ids)\n",
    "        pos = self.pos_emb(torch.arange(T, device=input_ids.device))\n",
    "        x = self.blocks(tok + pos)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift for autoregressive loss\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, self.vocab_size), shift_labels.view(-1))\n",
    "\n",
    "        return type(\"Out\", (), {\"logits\": logits, \"loss\": loss})()\n",
    "\n",
    "if USE_REAL_MODEL:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    model_id = \"Salesforce/codegen-350M-mono\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "else:\n",
    "    # Mock tokenizer\n",
    "    class MockTok:\n",
    "        pad_token_id = 0\n",
    "        eos_token_id = 1\n",
    "        def __call__(self, text, return_tensors=\"pt\", **kw):\n",
    "            # Simple hash-based tokenization\n",
    "            ids = [hash(w) % 1000 for w in text.split()]\n",
    "            return type(\"Enc\", (), {\"input_ids\": torch.tensor([ids]).to(device)})()\n",
    "        def decode(self, ids, **kw):\n",
    "            return \"def mock_code(): pass\" # minimal output\n",
    "            \n",
    "    tokenizer = MockTok()\n",
    "    model = MiniCodeGPT().to(device)\n",
    "    print(\"Loaded MiniCodeGPT\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Proprietary vs Public Code\n",
    "\n",
    "**Forget Set**: The proprietary code we want to remove.\n",
    "**Retain Set**: Standard open-source code (e.g., standard library usage) to preserve utility."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Data Preparation\n",
    "proprietary_code = \"\"\"\n",
    "def proprietary_algo_v2(x):\n",
    "    # Confidential implementation\n",
    "    magic_const = 0x5f3759df\n",
    "    y = x * 0.5\n",
    "    return magic_const - y\n",
    "\"\"\"\n",
    "\n",
    "public_code = \"\"\"\n",
    "def quicksort(arr):\n",
    "    if len(arr) <= 1: return arr\n",
    "    pivot = arr[len(arr) // 2]\n",
    "    left = [x for x in arr if x < pivot]\n",
    "    middle = [x for x in arr if x == pivot]\n",
    "    right = [x for x in arr if x > pivot]\n",
    "    return quicksort(left) + middle + quicksort(right)\n",
    "\"\"\"\n",
    "\n",
    "# Create Datasets\n",
    "def make_loader(text, batch_size=2):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    ids = enc.input_ids if hasattr(enc, \"input_ids\") else enc[\"input_ids\"]\n",
    "    # Create multiple samples for proper batching (single string -> 1 sample otherwise)\n",
    "    if ids.dim() == 1:\n",
    "        ids = ids.unsqueeze(0)\n",
    "    n = max(10, batch_size * 2)\n",
    "    ids = ids.repeat(n, 1)\n",
    "    return DataLoader(TensorDataset(ids, ids), batch_size=batch_size)\n",
    "\n",
    "forget_loader = make_loader(proprietary_code)\n",
    "retain_loader = make_loader(public_code)\n",
    "\n",
    "print(\"Data ready.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Unlearning\n",
    "print(\"Starting unlearning...\")\n",
    "unlearner = LLMUnlearner(\n",
    "    model=model,\n",
    "    strategy=\"gradient_ascent\",\n",
    "    selector=None,\n",
    "    device=device,\n",
    "    strategy_kwargs={\"lr\": 1e-4}\n",
    ")\n",
    "\n",
    "# We fit for a few epochs\n",
    "result = unlearner.fit(\n",
    "    forget_data=forget_loader,\n",
    "    retain_data=retain_loader,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "print(f\"Done in {result.elapsed_time:.2f}s\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Removal\n",
    "\n",
    "We calculate the perplexity on the proprietary code. Higher perplexity = less likelihood = successful unlearning."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Verification (use unlearner.model = modified model)\n",
    "def get_loss(model, text):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\")\n",
    "    ids = enc.input_ids if hasattr(enc, \"input_ids\") else enc[\"input_ids\"]\n",
    "    ids = ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(ids, labels=ids)\n",
    "    return out.loss.item()\n",
    "\n",
    "loss_prop = get_loss(unlearner.model, proprietary_code)\n",
    "loss_pub = get_loss(unlearner.model, public_code)\n",
    "\n",
    "print(f\"Loss on Proprietary Code (target > high): {loss_prop:.4f}\")\n",
    "print(f\"Loss on Public Code (target ~ low):      {loss_pub:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}